{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e0c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the file list for test and train sets.\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "names_train=['dataset/train_folder/'+a for a in os.listdir('dataset/train_folder')]\n",
    "names_test=['dataset/test__folder/'+a for a in os.listdir('dataset/test__folder')]\n",
    "names_query=['dataset/query/'+a for a in os.listdir('dataset/query')]\n",
    "class_names=[] # Put class names here.\n",
    "LOAD=False\n",
    "SPLIT=False\n",
    "query_idx=1 # It is used for test. If you use a query selected from the test set, you should set this variable to its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e8e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### LOAD from already saved files\n",
    "#####################################################\n",
    "if LOAD:\n",
    "    weights_of_blocks_train=np.load('./data/weights_of_blocks_trains_shuffle_HLF.npy')\n",
    "    # Convert all the NaNs into zero. I did not change the original data\n",
    "    weights_of_blocks_train=np.nan_to_num(weights_of_blocks_train) \n",
    "    im_p_test=names_test\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    df_test=pd.read_pickle('./data/df_test_features_HLF.pkl')\n",
    "\n",
    "    df_test=pd.read_pickle('./data/df_query_features_HLF.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0df07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "from Tools.Color_CDH import color_util\n",
    "from Tools.Gabor import Gabor_util\n",
    "from Tools.Blocking import Block_util\n",
    "\n",
    "import Tools.HNN_weights as HNNWB\n",
    "import Tools.Calculate_final_weight_HNN as HNNW\n",
    "from Tools.Block_feat_dataframe import Block_feat_dataframe_HLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4601610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "im_p_train=shuffle(names_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd944699",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(LOAD):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "    model = VGG16(weights='imagenet',include_top=True)\n",
    "    model=Model(inputs=model.input, outputs=model.get_layer('fc2').output)\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "\n",
    "    weights_of_blocks_train=HNNWB.HNN_blocks_weights_calculation_HLF(model,im_p=im_p_train,im_size=256,block_num=4,test_im=0,show_test=0,show_weights=False,save='Oxford5k.jpg')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b07b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(LOAD):\n",
    "    from numpy import save\n",
    "    #Convert all the NaNs to 0.0\n",
    "    weights_of_blocks_train=np.nan_to_num(weights_of_blocks_train)\n",
    "    save('./data/weights_of_blocks_trains_shuffle_HLF.npy', weights_of_blocks_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791423",
   "metadata": {},
   "source": [
    "Up to here, the feature are loaded and stored in weights_of_blocks_train.npy\n",
    "# Test set calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6826bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(LOAD):\n",
    "    im_p_test=names_test\n",
    "\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Do you need to recalculate the feature of the test set? if yes, run this cell. otherwise do not run this cell and load the pickle file.\n",
    "    #####################################################################################################################\n",
    "    import numpy as np\n",
    "    import Tools.HNN_weights as HNNWB\n",
    "    import Tools.Calculate_final_weight_HNN as HNNW\n",
    "    from Tools.Block_feat_dataframe import Block_feat_dataframe_HLF\n",
    "    start = time.process_time()\n",
    "    df_test,feature_shape_test=Block_feat_dataframe_HLF(model,im_p=im_p_test,im_size=256,block_num=4,test_im=0,show_test=0)\n",
    "    print(f'Calculating features for testing set took {time.process_time() - start} seconds')\n",
    "    df_test.to_pickle(\"./data/df_test_features_HLF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feda406",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(LOAD):\n",
    "    start = time.process_time()\n",
    "    im_p_query=names_query\n",
    "    df_query,feature_shape_query=Block_feat_dataframe_HLF(model,im_p=im_p_query,im_size=256,block_num=4,test_im=0,show_test=0)\n",
    "    print(f'Calculting features for queries took {time.process_time() - start} seconds')\n",
    "    df_query.to_pickle(\"./data/df_query_features_HLF.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996adc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    df_test=pd.read_pickle('./data/df_test_features_HLF.pkl')\n",
    "    df_query=pd.read_pickle('./data/df_query_features_HLF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31645a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def querying(P_M,query,number_image_to_return=2):\n",
    "\n",
    "    total_number_of_similar_images=0\n",
    "    P_M_sorted=P_M.copy(deep=True)\n",
    "    P_M_sorted=P_M_sorted.sort_values(by=\"l\",ascending=True,ignore_index=True)\n",
    "    P_M_sorted=P_M_sorted.sort_index()\n",
    "    for cls in class_names:\n",
    "        if cls in query.iloc[0,0]:\n",
    "            Classname=cls\n",
    "\n",
    "    for i in range(len(P_M_sorted.index)):\n",
    "        if Classname in P_M_sorted.at[i,'filename']:\n",
    "            total_number_of_similar_images+=1\n",
    "\n",
    "    start = int(round(time.time() * 1000000))\n",
    "\n",
    "\n",
    "    similar=0\n",
    "    for i in range(number_image_to_return):\n",
    "        if Classname in P_M_sorted.iloc[i,0]:\n",
    "            similar+=1\n",
    "    time_took=int(round(time.time() * 1000000)) - start\n",
    "        \n",
    "    precision=similar/number_image_to_return\n",
    "    recall=similar/total_number_of_similar_images\n",
    "    return precision,recall,time_took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c934ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [08:29<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "start = time.process_time()\n",
    "step=1\n",
    "# def cal_Pn(B,feat,W_b):\n",
    "feat_hat_test=df_test.copy(deep=True)\n",
    "feat_hat_query=df_query.copy(deep=True)\n",
    "\n",
    "number_image_to_return=50\n",
    "B=[0.5,0.5,0.5,0.5,0.5,2,2,0.5,0.5,2,2,0.5,0.5,0.5,0.5,0.5]\n",
    "\n",
    "numiter=100 # Number of iterations you want for the Hopfield to iterate. It should be >5\n",
    "feat_shape=np.shape(df_test.iloc[0,1])[0]\n",
    "num_B=np.shape(B)[0]\n",
    "num_image=df_test.shape\n",
    "num_image=num_image[0]\n",
    "num_image_query=df_query.shape\n",
    "num_image_query=num_image_query[0]\n",
    "\n",
    "P_M=pd.DataFrame(columns =['filename','P','s','l']) \n",
    "P_M_query=pd.DataFrame(columns =['filename','P','s','l']) \n",
    "\n",
    "for i in range(num_image):\n",
    "    P_M.at[i,\"filename\"]=df_test.at[i,\"filename\"]\n",
    "    P_M.at[i,\"P\"]=np.zeros([1,feat_shape])\n",
    "    P_M.at[i,\"s\"]=0\n",
    "    P_M.at[i,\"l\"]=0\n",
    "\n",
    "for i in range(num_image_query):\n",
    "    P_M_query.at[i,\"filename\"]=df_query.at[i,\"filename\"]\n",
    "    P_M_query.at[i,\"P\"]=np.zeros([1,feat_shape])\n",
    "    P_M_query.at[i,\"s\"]=0\n",
    "    P_M_query.at[i,\"l\"]=0\n",
    "\n",
    "\n",
    "feat_hat_p=np.zeros([num_image,num_B,feat_shape])\n",
    "feat_hat_p_query=np.zeros([num_image_query,num_B,feat_shape])\n",
    "\n",
    "for nIm in range(num_image):\n",
    "    P_M.at[nIm,'filename']=df_test.iloc[nIm,0]\n",
    "    for b in range(num_B):\n",
    "        feat_hat_p[nIm][b][:]=df_test.iloc[nIm][b+1]\n",
    "\n",
    "for nIm in range(num_image_query):\n",
    "    P_M_query.at[nIm,'filename']=df_query.iloc[nIm,0]\n",
    "    for b in range(num_B):\n",
    "        feat_hat_p_query[nIm][b][:]=df_query.iloc[nIm][b+1]\n",
    "\n",
    "feat_hat_org=feat_hat_p  # Just to test if the changed feature actually changed?\n",
    "\n",
    "feat_hat_cal=np.zeros([1,feat_shape])\n",
    "feat_hat_cal_query=np.zeros([1,feat_shape])\n",
    "for nIm in range(num_image):\n",
    "    for b in range(num_B):\n",
    "        feat_hat_cal=feat_hat_p[nIm][b][:]\n",
    "        for itr in range(numiter):\n",
    "            activations=np.matmul(weights_of_blocks_train[b],feat_hat_cal)\n",
    "        feat_hat_test.iloc[nIm,b+1]=B[b]*activations # I did this to make everything separated as much as possible for further change if required\n",
    "        P_M.at[nIm,'P']=P_M.at[nIm,'P'] +activations\n",
    "    P_M.at[nIm,'s']=sum(sum(P_M.at[nIm,'P']))\n",
    "\n",
    "\n",
    "for nIm in range(num_image_query):\n",
    "    for b in range(num_B):\n",
    "        feat_hat_cal_query=feat_hat_p_query[nIm][b][:]\n",
    "        for itr in range(numiter):\n",
    "            activations=np.matmul(weights_of_blocks_train[b],feat_hat_cal_query)\n",
    "        feat_hat_query.iloc[nIm,b+1]=B[b]*activations # I did this to make everything separated as much as possible for further change if required\n",
    "        P_M_query.at[nIm,'P']=P_M_query.at[nIm,'P'] +activations\n",
    "    P_M_query.at[nIm,'s']=sum(sum(P_M_query.at[nIm,'P']))\n",
    "\n",
    "#Start the calculation of |s^i-s^q|\n",
    "results_dic={}\n",
    "res_time_dif_num_query=pd.DataFrame(columns =['Number_ret','Time'])\n",
    "idx=0\n",
    "for number_image_to_return in tqdm(range(100,num_image+1,step)):\n",
    "    res=pd.DataFrame(columns =['query','Precision','Recall'])\n",
    "    for num_q in range(num_image_query):\n",
    "        res.at[num_q,'query']=P_M_query.at[num_q,'filename']\n",
    "        for nIm in range(num_image):\n",
    "            P_M.at[nIm,'l']=abs(P_M.at[nIm,'s']-P_M_query.at[num_q,'s'])\n",
    "\n",
    "        p,r,t=querying(P_M=P_M,query=P_M_query.iloc[num_q,:].to_frame().transpose(),number_image_to_return=number_image_to_return)\n",
    "        res.at[num_q,'Precision']=p\n",
    "        res.at[num_q,'Recall']=r\n",
    "    results_dic[idx]=res\n",
    "    finish_t=time.process_time() - start\n",
    "    res_time_dif_num_query.at[idx,'Number_ret']=number_image_to_return\n",
    "    res_time_dif_num_query.at[idx,'Time']=finish_t\n",
    "    idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import DCT\n",
    "from Tools import DWT\n",
    "from Tools import BST\n",
    "def hash_cal(imp):\n",
    "    wSlid_DCT=DCT.windows_comput(imp)\n",
    "    binCode_DCT=DCT.DCT_hash_compute(wSlid_DCT)\n",
    "    hashCode_DCT=DCT.bin_dec(binCode_DCT)\n",
    "    \n",
    "    wSlid_DWT=DWT.windows_comput(imp,block_num=8)\n",
    "    binCode_DWT=DWT.DWT_hash_compute(wSlid_DWT)\n",
    "    hashCode_DWT=DWT.bin_dec(binCode_DWT)\n",
    "    binCode_DWTp1,binCode_DWTp2=DWT.bin_two_parts(binCode_DWT)\n",
    "    hashCode_DWTp1=DWT.bin_dec(binCode_DWTp1)\n",
    "    hashCode_DWTp2=DWT.bin_dec(binCode_DWTp2)\n",
    "    return hashCode_DCT,hashCode_DWTp1,hashCode_DWTp2,hashCode_DWT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hash_testset = pd.DataFrame(columns=['index', 'filename', 'DCT','DWT_L','DWT_R','DWT'])\n",
    "im_path=names_test\n",
    "idx=0\n",
    "for imp in tqdm(list_of_images):\n",
    "    hashCode_DCT,hashCode_DWTp1,hashCode_DWTp2,hashCode_DWT=hash_cal(imp)\n",
    "    df_hash_testset = df_hash_testset.append({'index': idx, 'filename':imp,'DCT':hashCode_DCT,\n",
    "                                                'DWT_L':hashCode_DWTp1,'DWT_R':hashCode_DWTp2,'DWT':hashCode_DWT}, ignore_index=True)\n",
    "    idx += 1\n",
    "\n",
    "hashCode_DCT_query,hashCode_DWTp1_query,hashCode_DWTp2_query,hashCode_DWT_query=hash_cal(P_M.iloc[query,0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "320329f7123b61ff33a61fb717ebd18434750ebede8b02b018e624b88a62487b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
