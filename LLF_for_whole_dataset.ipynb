{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e0c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "names_train=['dataset/train_folder/'+a for a in os.listdir('dataset/train_folder')]\n",
    "names_test=['dataset/test_folder/'+a for a in os.listdir('dataset/test_folder')]\n",
    "names_query=['dataset/query/'+a for a in os.listdir('dataset/query')]\n",
    "class_names=[] # You should put the class names in this list\n",
    "LOAD=False\n",
    "query_idx=1 # It is used for test. If you use a query selected from the test set, you should set this variable to its index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e8e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### LOAD from already saved files\n",
    "#####################################################\n",
    "if LOAD:\n",
    "    weights_of_blocks_train=np.load('./data/weights_of_blocks_trains_shuffle.npy')\n",
    "    # Convert all the NaNs into zero. I did not change the original data\n",
    "    weights_of_blocks_train=np.nan_to_num(weights_of_blocks_train) \n",
    "    im_p_test=names_test\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    df_test=pd.read_pickle('./data/df_test_features.pkl')\n",
    "\n",
    "    df_test=pd.read_pickle('./data/df_query_features.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0df07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "from Tools.Color_CDH import color_util\n",
    "from Tools.Gabor import Gabor_util\n",
    "from Tools.Blocking import Block_util\n",
    "\n",
    "import Tools.HNN_weights as HNNWB\n",
    "import Tools.Calculate_final_weight_HNN as HNNW\n",
    "from Tools.Block_feat_dataframe import Block_feat_dataframe_LLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4601610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "im_p_train=shuffle(names_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd944699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 18:46:49\n",
      "dataset/train_folder/all_souls_000003.jpg\n",
      "Calculating Features for Block # 16       Image # 0 is done!\n",
      "dataset/train_folder/all_souls_000005.jpg\n",
      "Calculating Features for Block # 16       Image # 1 is done!\n",
      "dataset/train_folder/all_souls_000001.jpg\n",
      "Calculating Features for Block # 16       Image # 2 is done!\n",
      "dataset/train_folder/ashmolean_000003.jpg\n",
      "Calculating Features for Block # 16       Image # 3 is done!\n",
      "dataset/train_folder/all_souls_000002.jpg\n",
      "Calculating Features for Block # 16       Image # 4 is done!\n",
      "No errors so far! You are an awesome programmer.\n",
      "The feature set needs 19968 bytes\n",
      "There are 1408 zeros out of 2496\n",
      "Calculation of the weight matrix is done for 5 images. Shape of Block_weight is (16, 156, 156)\n",
      "Number of errors in G1 is 0  and for G2 is 0\n",
      "Calculating the weight matrix took 21.71875 seconds\n",
      "End Time = 18:47:10\n"
     ]
    }
   ],
   "source": [
    "if not(LOAD):\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now()\n",
    "\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Start Time =\", current_time)\n",
    "    start = time.process_time()\n",
    "\n",
    "    weights_of_blocks_train,g1err_total,g2err_total,NaNerr_total,df_err=HNNWB.HNN_blocks_weights_calculation_LLF(im_p_train,im_size=256,block_num=4,test_im=0,show_test=0,show_weights=False,save='Oxford5k.jpg')\n",
    "    print(f\"Number of errors in G1 is {g1err_total}  and for G2 is {g2err_total}\")\n",
    "    print(f'Calculating the weight matrix took {time.process_time() - start} seconds')\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"End Time =\", current_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b07b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(LOAD):\n",
    "    from numpy import save\n",
    "    #Convert all the NaNs to 0.0\n",
    "    weights_of_blocks_train=np.nan_to_num(weights_of_blocks_train)\n",
    "    save('./data/weights_of_blocks_trains_shuffle.npy', weights_of_blocks_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791423",
   "metadata": {},
   "source": [
    "Up to here, the feature are loaded and stored in weights_of_blocks_train.npy\n",
    "# Test set calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6826bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/test_folder/all_souls_000000.jpg\n",
      "Calculating Features for Block # 16       Image # 0 is done!\n",
      "dataset/test_folder/all_souls_000001.jpg\n",
      "Calculating Features for Block # 16       Image # 1 is done!\n",
      "dataset/test_folder/all_souls_000002.jpg\n",
      "Calculating Features for Block # 16       Image # 2 is done!\n",
      "dataset/test_folder/all_souls_000003.jpg\n",
      "Calculating Features for Block # 16       Image # 3 is done!\n",
      "dataset/test_folder/all_souls_000005.jpg\n",
      "Calculating Features for Block # 16       Image # 4 is done!\n",
      "No errors so far! You are an awesome programmer.\n",
      "The feature set needs 19968 bytes\n",
      "There are 1436 zeros out of 2496\n",
      "Calculating features for testing set took 21.265625 seconds\n"
     ]
    }
   ],
   "source": [
    "if not(LOAD):\n",
    "    im_p_test=names_test\n",
    "\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # Do you need to recalculate the feature of the test set? if yes, run this cell. otherwise do not run this cell and load the pickle file.\n",
    "    #####################################################################################################################\n",
    "    import numpy as np\n",
    "    import Tools.HNN_weights as HNNWB\n",
    "    import Tools.Calculate_final_weight_HNN as HNNW\n",
    "    from Tools.Block_feat_dataframe import Block_feat_dataframe_LLF\n",
    "    start = time.process_time()\n",
    "    df_test,feature_shape_test,g1err,g2err,e1,e2=Block_feat_dataframe_LLF(im_p_test,im_size=256,block_num=4,test_im=0,show_test=0)\n",
    "    print(f'Calculating features for testing set took {time.process_time() - start} seconds')\n",
    "\n",
    "    #Save df_test containing all the features into a file\n",
    "    df_test.to_pickle(\"./data/df_test_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0feda406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/query/all_souls_000000.jpg\n",
      "Calculating Features for Block # 16       Image # 0 is done!\n",
      "No errors so far! You are an awesome programmer.\n",
      "The feature set needs 19968 bytes\n",
      "There are 1395 zeros out of 2496\n",
      "Calculting features for queries took 4.53125 seconds\n"
     ]
    }
   ],
   "source": [
    "if not(LOAD):\n",
    "    start = time.process_time()\n",
    "    im_p_query=names_query\n",
    "    df_query,feature_shape_query,_,_,_,_=Block_feat_dataframe_LLF(im_p_query,im_size=256,block_num=4,test_im=0,show_test=0)\n",
    "    print(f'Calculting features for queries took {time.process_time() - start} seconds')\n",
    "    df_query.to_pickle(\"./data/df_query_features.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "996adc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    df_test=pd.read_pickle('./data/df_test_features.pkl')\n",
    "    df_query=pd.read_pickle('./data/df_query_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31645a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def querying(P_M,query,number_image_to_return=2):\n",
    "\n",
    "    total_number_of_similar_images=0\n",
    "    P_M_sorted=P_M.copy(deep=True)\n",
    "    P_M_sorted=P_M_sorted.sort_values(by=\"l\",ascending=True,ignore_index=True)\n",
    "    P_M_sorted=P_M_sorted.sort_index()\n",
    "    for cls in class_names:\n",
    "        if cls in query.iloc[0,0]:\n",
    "            Classname=cls\n",
    "\n",
    "    for i in range(len(P_M_sorted.index)):\n",
    "        if Classname in P_M_sorted.at[i,'filename']:\n",
    "            total_number_of_similar_images+=1\n",
    "\n",
    "    start = int(round(time.time() * 1000000))\n",
    "\n",
    "\n",
    "    similar=0\n",
    "    list_of_images=['']\n",
    "    for i in range(number_image_to_return):\n",
    "        list_of_images=np.vstack((list_of_images,P_M_sorted.iloc[i,0]))\n",
    "\n",
    "        if Classname in P_M_sorted.iloc[i,0]:\n",
    "            similar+=1\n",
    "\n",
    "    time_took=int(round(time.time() * 1000000)) - start\n",
    "        \n",
    "    precision=similar/number_image_to_return\n",
    "    recall=similar/total_number_of_similar_images\n",
    "    list_of_images=np.delete(list_of_images,0) # Removing the first empty folder name because of initilizing it with ''\n",
    "\n",
    "    return precision,recall,time_took,list_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c934ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 334.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "start = time.process_time()\n",
    "step=100\n",
    "# def cal_Pn(B,feat,W_b):\n",
    "feat_hat_test=df_test.copy(deep=True)\n",
    "feat_hat_query=df_query.copy(deep=True)\n",
    "\n",
    "number_image_to_return=50\n",
    "B=[0.5,0.5,0.5,0.5,0.5,2,2,0.5,0.5,2,2,0.5,0.5,0.5,0.5,0.5]\n",
    "\n",
    "numiter=10 # Number of iterations you want for the Hopfield to iterate. It should be >5\n",
    "feat_shape=np.shape(df_test.iloc[0,1])[0]\n",
    "num_B=np.shape(B)[0]\n",
    "num_image=df_test.shape\n",
    "num_image=num_image[0]\n",
    "num_image_query=df_query.shape\n",
    "num_image_query=num_image_query[0]\n",
    "\n",
    "P_M=pd.DataFrame(columns =['filename','P','s','l']) \n",
    "P_M_query=pd.DataFrame(columns =['filename','P','s','l']) \n",
    "\n",
    "for i in range(num_image):\n",
    "    P_M.at[i,\"filename\"]=df_test.at[i,\"filename\"]\n",
    "    P_M.at[i,\"P\"]=np.zeros([1,feat_shape])\n",
    "    P_M.at[i,\"s\"]=0\n",
    "    P_M.at[i,\"l\"]=0\n",
    "\n",
    "for i in range(num_image_query):\n",
    "    P_M_query.at[i,\"filename\"]=df_query.at[i,\"filename\"]\n",
    "    P_M_query.at[i,\"P\"]=np.zeros([1,feat_shape])\n",
    "    P_M_query.at[i,\"s\"]=0\n",
    "    P_M_query.at[i,\"l\"]=0\n",
    "\n",
    "\n",
    "feat_hat_p=np.zeros([num_image,num_B,feat_shape])\n",
    "feat_hat_p_query=np.zeros([num_image_query,num_B,feat_shape])\n",
    "\n",
    "for nIm in range(num_image):\n",
    "    P_M.at[nIm,'filename']=df_test.iloc[nIm,0]\n",
    "    for b in range(num_B):\n",
    "        feat_hat_p[nIm][b][:]=df_test.iloc[nIm][b+1]\n",
    "\n",
    "for nIm in range(num_image_query):\n",
    "    P_M_query.at[nIm,'filename']=df_query.iloc[nIm,0]\n",
    "    for b in range(num_B):\n",
    "        feat_hat_p_query[nIm][b][:]=df_query.iloc[nIm][b+1]\n",
    "\n",
    "feat_hat_org=feat_hat_p  # Just to test if the changed feature actually changed?\n",
    "\n",
    "feat_hat_cal=np.zeros([1,feat_shape])\n",
    "feat_hat_cal_query=np.zeros([1,feat_shape])\n",
    "for nIm in range(num_image):\n",
    "    for b in range(num_B):\n",
    "        feat_hat_cal=feat_hat_p[nIm][b][:]\n",
    "        for itr in range(numiter):\n",
    "            activations=np.matmul(weights_of_blocks_train[b],feat_hat_cal)\n",
    "        feat_hat_test.iloc[nIm,b+1]=B[b]*activations # I did this to make everything separated as much as possible for further change if required\n",
    "        P_M.at[nIm,'P']=P_M.at[nIm,'P'] +activations\n",
    "    P_M.at[nIm,'s']=sum(sum(P_M.at[nIm,'P']))\n",
    "\n",
    "\n",
    "for nIm in range(num_image_query):\n",
    "    for b in range(num_B):\n",
    "        feat_hat_cal_query=feat_hat_p_query[nIm][b][:]\n",
    "        for itr in range(numiter):\n",
    "            activations=np.matmul(weights_of_blocks_train[b],feat_hat_cal_query)\n",
    "        feat_hat_query.iloc[nIm,b+1]=B[b]*activations # I did this to make everything separated as much as possible for further change if required\n",
    "        P_M_query.at[nIm,'P']=P_M_query.at[nIm,'P'] +activations\n",
    "    P_M_query.at[nIm,'s']=sum(sum(P_M_query.at[nIm,'P']))\n",
    "\n",
    "#Start the calculation of |s^i-s^q|\n",
    "results_dic={}\n",
    "res_time_dif_num_query=pd.DataFrame(columns =['Number_ret','Time'])\n",
    "idx=0\n",
    "for number_image_to_return in tqdm(range(1,num_image+1,step)):\n",
    "    res=pd.DataFrame(columns =['query','Precision','Recall'])\n",
    "    for num_q in range(num_image_query):\n",
    "        res.at[num_q,'query']=P_M_query.at[num_q,'filename']\n",
    "        for nIm in range(num_image):\n",
    "            P_M.at[nIm,'l']=abs(P_M.at[nIm,'s']-P_M_query.at[num_q,'s'])\n",
    "\n",
    "        p,r,t,list_of_images=querying(P_M=P_M,query=P_M_query.iloc[num_q,:].to_frame().transpose(),number_image_to_return=number_image_to_return)\n",
    "        res.at[num_q,'Precision']=p\n",
    "        res.at[num_q,'Recall']=r\n",
    "    results_dic[idx]=res\n",
    "    finish_t=time.process_time() - start\n",
    "    res_time_dif_num_query.at[idx,'Number_ret']=number_image_to_return\n",
    "    res_time_dif_num_query.at[idx,'Time']=finish_t\n",
    "    idx+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import DCT\n",
    "from Tools import DWT\n",
    "from Tools import BST\n",
    "def hash_cal(imp):\n",
    "    wSlid_DCT=DCT.windows_comput(imp)\n",
    "    binCode_DCT=DCT.DCT_hash_compute(wSlid_DCT)\n",
    "    hashCode_DCT=DCT.bin_dec(binCode_DCT)\n",
    "    \n",
    "    wSlid_DWT=DWT.windows_comput(imp,block_num=8)\n",
    "    binCode_DWT=DWT.DWT_hash_compute(wSlid_DWT)\n",
    "    hashCode_DWT=DWT.bin_dec(binCode_DWT)\n",
    "    binCode_DWTp1,binCode_DWTp2=DWT.bin_two_parts(binCode_DWT)\n",
    "    hashCode_DWTp1=DWT.bin_dec(binCode_DWTp1)\n",
    "    hashCode_DWTp2=DWT.bin_dec(binCode_DWTp2)\n",
    "    return hashCode_DCT,hashCode_DWTp1,hashCode_DWTp2,hashCode_DWT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebaa3cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_hash_testset = pd.DataFrame(columns=['index', 'filename', 'DCT','DWT_L','DWT_R','DWT'])\n",
    "im_path=names_test\n",
    "idx=0\n",
    "start = time.process_time()\n",
    "\n",
    "for imp in tqdm(list_of_images):\n",
    "    hashCode_DCT,hashCode_DWTp1,hashCode_DWTp2,hashCode_DWT=hash_cal(imp)\n",
    "    df_hash_testset = df_hash_testset.append({'index': idx, 'filename':imp,'DCT':hashCode_DCT,\n",
    "                                                'DWT_L':hashCode_DWTp1,'DWT_R':hashCode_DWTp2,'DWT':hashCode_DWT}, ignore_index=True)\n",
    "    idx += 1\n",
    "finish_t=time.process_time() - start\n",
    "\n",
    "# Instead of using query_idx, you should use query datafram df_query if there are more than one images or you are not using one (or more) images ...\n",
    "# from the test set as queries. In that case, you should do a similar alogirthm as lines 8-12. Then you need to alter below cells a little to match  it.\n",
    "# I prefer to clean the code by putting one query here. I found the automatic way reduces the readibility of the code.\n",
    "hashCode_DCT_query,hashCode_DWTp1_query,hashCode_DWTp2_query,hashCode_DWT_query=hash_cal(P_M.iloc[query_idx,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d7f0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_in_tree(hashCode_query,BST,list_H,thresh):\n",
    "        \n",
    "    lookfor=0\n",
    "    for i in range(len(list_H)):\n",
    "        cal=hashCode_query-list_H[i]\n",
    "\n",
    "        if cal<thresh and cal!=0:\n",
    "            lookfor=list_H[i+50]\n",
    "            #print ('Found in index=',i)\n",
    "            return BST.findsuccessorforN(lookfor).inorder([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6666985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree building is done in 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\ARP_GIT\\LLF_for_whole_dataset.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m thresh_DWTp2\u001b[39m=\u001b[39mhashCode_DWTp2_query\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m thresh_DWT\u001b[39m=\u001b[39mhashCode_DWT_query\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m delta1\u001b[39m=\u001b[39mlook_in_tree(hashCode_DCT_query,BST_DCT,DCT_inorder,thresh_DCT)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#print(len(delta1))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m delta2\u001b[39m=\u001b[39mlook_in_tree(hashCode_DWTp1_query,BST_DWT1,DWTp1_inorder,thresh_DWTp1)\n",
      "\u001b[1;32mg:\\ARP_GIT\\LLF_for_whole_dataset.ipynb Cell 16\u001b[0m in \u001b[0;36mlook_in_tree\u001b[1;34m(hashCode_query, BST, list_H, thresh)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cal\u001b[39m=\u001b[39mhashCode_query\u001b[39m-\u001b[39mlist_H[i]\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m cal\u001b[39m<\u001b[39mthresh \u001b[39mand\u001b[39;00m cal\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     lookfor\u001b[39m=\u001b[39mlist_H[i\u001b[39m+\u001b[39;49m\u001b[39m50\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#print ('Found in index=',i)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/ARP_GIT/LLF_for_whole_dataset.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m BST\u001b[39m.\u001b[39mfindsuccessorforN(lookfor)\u001b[39m.\u001b[39minorder([])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "t1=t2=30\n",
    "t3=20\n",
    "for i in tqdm(range(1)):\n",
    "    start = time.process_time()\n",
    "    df=df_hash_testset\n",
    "    df_hash_testset_shuffle = df.sample(frac = 1,ignore_index=True)\n",
    "    df_hash_testset_shuffle=df_hash_testset_shuffle.sort_index()\n",
    "    df_hash_testset_shuffle.to_pickle('1.pkl')\n",
    "    BST_DCT=BST.BSTNode()\n",
    "    BST_DWT1=BST.BSTNode()\n",
    "    BST_DWT2=BST.BSTNode()\n",
    "    BST_DWT=BST.BSTNode()\n",
    "    for i in range(len(df)):\n",
    "        BST_DCT.insert(df_hash_testset_shuffle.at[i,'DCT'])\n",
    "        BST_DWT1.insert(df_hash_testset_shuffle.at[i,'DWT_L'])\n",
    "        BST_DWT2.insert(df_hash_testset_shuffle.at[i,'DWT_R'])\n",
    "        BST_DWT.insert(df_hash_testset_shuffle.at[i,'DWT_R'])\n",
    "\n",
    "    finish_t=time.process_time() - start\n",
    "    print(f\"tree building is done in {finish_t}\")\n",
    "\n",
    "    start = time.process_time()\n",
    "\n",
    "    DCT_inorder=BST_DCT.inorder([])\n",
    "    DWTp1_inorder=BST_DWT1.inorder([])\n",
    "    DWTp2_inorder=BST_DWT2.inorder([])\n",
    "    DWT_inorder=BST_DWT.inorder([])\n",
    "\n",
    "    thresh_DCT=hashCode_DCT_query\n",
    "    thresh_DWTp1=hashCode_DWTp1_query\n",
    "    thresh_DWTp2=hashCode_DWTp2_query\n",
    "    thresh_DWT=hashCode_DWT_query\n",
    "    delta1=look_in_tree(hashCode_DCT_query,BST_DCT,DCT_inorder,thresh_DCT)\n",
    "    #print(len(delta1))\n",
    "    delta2=look_in_tree(hashCode_DWTp1_query,BST_DWT1,DWTp1_inorder,thresh_DWTp1)\n",
    "    #print(len(delta2))\n",
    "    delta3=look_in_tree(hashCode_DWTp2_query,BST_DWT2,DWTp2_inorder,thresh_DWTp2)\n",
    "    #print(len(delta3))\n",
    "    delta4=look_in_tree(hashCode_DWT_query,BST_DWT,DWT_inorder,thresh_DWT)\n",
    "    #print(len(delta4))\n",
    "    finish_t=time.process_time() - start\n",
    "    print(f\"look up is done in {finish_t}\")\n",
    "    dct_size=sys.getsizeof(BST_DCT)\n",
    "    print(f\"size of DCT tree is {dct_size}\")\n",
    "    dwt1_size=sys.getsizeof(BST_DWT1)\n",
    "    print(f\"size of DCT tree is {dwt1_size}\")\n",
    "    dwt2_size=sys.getsizeof(BST_DWT2)\n",
    "    print(f\"size of DCT tree is {dwt2_size}\")\n",
    "    dwt_size=sys.getsizeof(BST_DWT)\n",
    "    print(f\"size of DCT tree is {dwt_size}\")\n",
    "    if (len(delta1)>t1 and len(delta2)>t2 and len(delta3)>t3):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('arp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "320329f7123b61ff33a61fb717ebd18434750ebede8b02b018e624b88a62487b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
